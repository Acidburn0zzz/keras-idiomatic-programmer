{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idiomatic Programmer Code Labs\n",
    "\n",
    "## Code Labs #4 - Get Familiar Advanced CNN Designs\n",
    "\n",
    "## Prerequistes:\n",
    "\n",
    "    1. Familiar with Python\n",
    "    2. Completed Handbook 4/Part 4: Advanced Convolutional Neural Networks\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "    1. Architecture Changes - Pre-stems\n",
    "    2. Dense connections across sublayers in DenseNet\n",
    "    3. Xception Redesigned Macro-Architecture for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Stems Groups for Handling Different Input Sizes\n",
    "\n",
    "Let's create a pre-stem to handle an input size different than what the neural network was designed for.\n",
    "\n",
    "We will use these approaches:\n",
    "\n",
    "    1. Calculate the difference in size between the expected input and the actual size of\n",
    "       the input (in our case we are assuming actual size less than expected size).\n",
    "       A. Expected = (230, 230, 3)\n",
    "       B. Actual   = (224, 224, 3)\n",
    "    2. Pad the inputs to fit into the expected size.\n",
    "    \n",
    "You fill in the blanks (replace the ??), make sure it passes the Python interpreter, and then verify it's correctness with the summary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Input\n",
    "\n",
    "# not the input shape expected by the stem (which is (230, 230, 3)\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Add a pre-stem and pad (224, 224, 3) to (230, 230, 3)\n",
    "# Hint: Since the pad is on both sides (left/right, top/bottom) you want to divide the\n",
    "# difference by two (half goes to the left, half goes to the right, etc)\n",
    "inputs = layers.ZeroPadding2D(??)(inputs)\n",
    "\n",
    "# this stem's expected shape is (230, 230, 3)\n",
    "x = layers.Conv2D(64, (7, 7), strides=(2,2))(inputs)\n",
    "X = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that actual is padded to expected:\n",
    "\n",
    "You should get the following output on the shape of the inputs and outputs\n",
    "\n",
    "```\n",
    "inputs (?, 230, 230, 3)\n",
    "outputs (?, 112, 112, 64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs (?, 230, 230, 3)\n",
      "outputs (?, 112, 112, 64)\n"
     ]
    }
   ],
   "source": [
    "# this will output: (230, 230, 3)\n",
    "print(\"inputs\", inputs.shape)\n",
    "\n",
    "# this will output: (?, 112, 112, 64)\n",
    "print(\"outputs\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet Group as Function API\n",
    "\n",
    "Let's create a DenseNet group:\n",
    "\n",
    "We will use these approaches:\n",
    "\n",
    "    1. TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Input, Model\n",
    "\n",
    "def stem(inputs):\n",
    "    \"\"\" The Stem Convolution Group\n",
    "        inputs : input tensor\n",
    "    \"\"\"\n",
    "    # First large convolution for abstract features for input 230 x 230 and output\n",
    "    # 112 x 112\n",
    "    x = layers.Conv2D(64, (7, 7), strides=2)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # Add padding so when downsampling we fit shape 56 x 56\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=2)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nblocks, nb_filters):\n",
    "    \"\"\" Construct a Dense Block\n",
    "        x         : input layer\n",
    "        nblocks   : number of residual blocks in dense block\n",
    "        nb_filters: number of filters in convolution layer in residual block\n",
    "    \"\"\"\n",
    "    # Construct a group of residual blocks\n",
    "    for _ in range(nblocks):\n",
    "        x = residual_block(x, nb_filters)\n",
    "    return x\n",
    "\n",
    "def residual_block(x, nb_filters):\n",
    "    \"\"\" Construct Residual Block\n",
    "        x         : input layer\n",
    "        nb_filters: number of filters in convolution layer in residual block\n",
    "    \"\"\"\n",
    "    shortcut = x # remember input tensor into residual block\n",
    "\n",
    "    # Bottleneck convolution, expand filters by 4 (DenseNet-B)\n",
    "    x = layers.Conv2D(4 * nb_filters, (1, 1), strides=(1, 1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3 x 3 convolution with padding=same to preserve same shape of feature maps\n",
    "    x = layers.Conv2D(nb_filters, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Concatenate the input (identity) with the output of the residual block\n",
    "    # Concatenation (vs. merging) provides Feature Reuse between layers\n",
    "    x = layers.concatenate([shortcut, x])\n",
    "    return x\n",
    "\n",
    "def trans_block(x, reduce_by):\n",
    "    \"\"\" Construct a Transition Block\n",
    "        x        : input layer\n",
    "        reduce_by: percentage of reduction of feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    # Reduce (compression) the number of feature maps (DenseNet-C)\n",
    "    # shape[n] returns a class object. We use int() to cast it into the dimension\n",
    "    # size\n",
    "    nb_filters = int( int(x.shape[3]) * reduce_by )\n",
    "\n",
    "    # Bottleneck convolution\n",
    "    x = layers.Conv2D(nb_filters, (1, 1), strides=(1, 1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Use mean value (average) instead of max value sampling when pooling\n",
    "    # reduce by 75%\n",
    "    x = layers.AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "    return x\n",
    "\n",
    "inputs = Input(shape=(230, 230, 3))\n",
    "\n",
    "# Create the Stem Convolution Group\n",
    "x = stem(inputs)\n",
    "\n",
    "# number of residual blocks in each dense block\n",
    "blocks = [6, 12, 24, 16]\n",
    "\n",
    "# pop off the list the last dense block\n",
    "last   = blocks.pop()\n",
    "\n",
    "# amount to reduce feature maps by (compression) during transition blocks\n",
    "reduce_by = 0.5\n",
    "\n",
    "# number of filters in a convolution block within a residual block\n",
    "nb_filters = 32\n",
    "\n",
    "# Create the dense blocks and interceding transition blocks\n",
    "for nblocks in blocks:\n",
    "    x = dense_block(x, nblocks, nb_filters)\n",
    "    x = trans_block(x, reduce_by)\n",
    "\n",
    "# Add the last dense block w/o a following transition block\n",
    "x = dense_block(x, last, nb_filters)\n",
    "\n",
    "# Classifier\n",
    "# Global Average Pooling will flatten the 7x7 feature maps into 1D feature maps\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# Fully connected output layer (classification)\n",
    "x = layers.Dense(1000, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
