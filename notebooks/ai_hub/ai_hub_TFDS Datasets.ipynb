{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFDS Datasets\n",
    "\n",
    "### Topic\n",
    "\n",
    "This notebook covers accessing and examining the TF builtin dataset. The datasets cover the categories of:\n",
    "\n",
    "    - Vision (CV)\n",
    "        - Binary Classification (BC)\n",
    "        - Multiple Class Classification (MC)\n",
    "        - Multiple Class Multiple Label Classification (MCML)\n",
    "        - Object Detection (OD)\n",
    "        - Video Intelligence (VI)\n",
    "    - Natural Language Processing (NLP)\n",
    "        - Text Classification (TC)\n",
    "        - Sentiment (SN)\n",
    "        - Entity Recognition (ER)\n",
    "        - Translation (TR)\n",
    "    - Structured (ST)\n",
    "    - Audio (AU)\n",
    "    \n",
    "### Audience\n",
    "\n",
    "The audience for this notebook are data scientists and machine learning engineers.\n",
    "\n",
    "### Prerequistes\n",
    "\n",
    "One should be familar with:\n",
    "\n",
    "    - Python 3.X\n",
    "    - Deep Learning\n",
    "    - Tensorflow\n",
    "    \n",
    "    ### Dataset\n",
    "\n",
    "This notebook uses builtin datasets which are part of Tensorflow Datasets:\n",
    "\n",
    "https://www.tensorflow.org/datasets/api_docs/python/tfds\n",
    "    \n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of this tutorial is to learn what are and how to use the TFDS builtin datasets.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial does not use any billable items.\n",
    "\n",
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or AI Platform Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "* Tensorflow\n",
    "* TFDS\n",
    "* Numpy\n",
    "* Matplotlib\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "2. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3.\n",
    "   \n",
    "3. [Install Tensorflow](https://www.tensorflow.org/install/pip)\n",
    "\n",
    "4. [Install Tensorflow Datasets](https://pypi.org/project/tensorflow-datasets)\n",
    "\n",
    "5. [Install Numpy](https://pypi.org/project/numpy/)\n",
    "\n",
    "6. [Install Matplotlib for Python](https://pypi.org/project/matplotlib/)\n",
    "\n",
    "7. Activate that environment and run `pip install jupyter` in a shell to install\n",
    "   Jupyter.\n",
    "\n",
    "8. Run `jupyter notebook` in a shell to launch Jupyter.\n",
    "\n",
    "9. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will be using the following Python modules:\n",
    "\n",
    "    - tensorflow (TF)    : Deep Learning Framework\n",
    "    - tensorflow_datasets: TF builtin datasets\n",
    "    - numpy              : Scientific/Math libraries for Arrays/Matrices/Tensors\n",
    "    - matplotlib         : Plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs\n",
    "\n",
    "You will need to install the following packages (which are not pre-installed on AI-Platform or Codelab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install apache_beam\n",
    "\n",
    "%pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run locally, you may need to install these additional modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_datasets\n",
    "%pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "### Imports\n",
    "\n",
    "Now import the python libraries we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify you are using TF 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Dataset Category/Subcategory\n",
    "\n",
    "Use the cell below to specify the category and subcategory for a dataset.\n",
    "\n",
    "    | Category   | Abbr | Subcategory                | Abbr |\n",
    "    | Vision     | CV   | Binary Classification      | BC   |\n",
    "    |            |      | Multi Classification       | MC   |\n",
    "    |            |      | Multi-Label Classification | ML   |\n",
    "    |            |      | Object Detection           | OD   |\n",
    "    |            |      | Video Intelligence         | VI   |\n",
    "    | Language   | NL   | Text Classification        | TC   |\n",
    "    |            |      | Sentiment Analysis         | SN   |\n",
    "    |            |      | Entity Recognition         | ER   |\n",
    "    |            |      | Translation                | TR   |\n",
    "    | Structured | ST   |  \n",
    "    | Audio      | AU   |                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Level Categories\n",
    "CATEGORIES=['AU', 'CV', 'NL', 'ST']\n",
    "\n",
    "# Select a category (CV by default)\n",
    "CATEGORY='CV'\n",
    "\n",
    "if CATEGORY == 'CV':\n",
    "    SUBCATEGORIES=[ 'BC', 'MC', 'ML', 'OD', 'VI']\n",
    "    # Select a subcategory (BC by default)\n",
    "    SUBCATEGORY='BC'\n",
    "elif CATEGORY == 'NL':\n",
    "    # Select a subcategory (TC by default)\n",
    "    SUBCATEGORIES=[ 'TC', 'SN', 'ER', 'TR']\n",
    "    SUBCATEGORY='TC'\n",
    "elif CATEGORY == 'ST':\n",
    "    SUBCATEGORY=''\n",
    "elif CATEGORY == 'AU':\n",
    "    SUBCATEGORY=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Now, let's use TFDS to load the selected builtin dataset. The dataset is downloaded into memory using `tfds.load()` method. We added setting the keyword parameter `with_info=True` to include downloading metadata (information) about the dataset.\n",
    "\n",
    "Once the dataset is downloaded, it is automatically shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our memory footprint before the download.\n",
    "!free -m\n",
    "\n",
    "# Load the Dataset from builtin source\n",
    "if CATEGORY == 'CV':\n",
    "    if SUBCATEGORY == 'BC':\n",
    "        name = 'cats_vs_dogs'\n",
    "    elif SUBCATEGORY == 'MC':\n",
    "        name = 'rock_paper_scissors'\n",
    "    elif SUBCATEGORY == 'ML':\n",
    "        # ANDY: Does not yet work.\n",
    "        name = 'bigearthnet'\n",
    "    elif SUBCATEGORY == 'OD':\n",
    "        name = 'voc2007'\n",
    "    elif SUBCATEGORY == 'VI':\n",
    "        name = 'ucf101'\n",
    "\n",
    "elif CATEGORY == 'NL':\n",
    "    if SUBCATEGORY == 'TC':\n",
    "        name = 'snli'\n",
    "    elif SUBCATEGORY == 'SN':\n",
    "        name = 'imdb_reviews'\n",
    "    elif SUBCATEGORY == 'ER':\n",
    "        pass\n",
    "    elif SUBCATEGORY == 'TR':\n",
    "        name = 'para_crawl'\n",
    "        \n",
    "elif CATEGORY == 'ST':\n",
    "    name = 'iris'\n",
    "elif CATEGORY == 'AU':\n",
    "    name = 'nsynth'\n",
    "\n",
    "data, info = tfds.load(name, with_info=True)\n",
    "\n",
    "# Let's look at our memory footprint after the download.\n",
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "Now let's display (print) the dataset's metadata (information on the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset information\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "\n",
    "Some of the datasets come presplit into train and test (holdout), while others are stored as a single dataset (not split). \n",
    "\n",
    "In either case, the training data and/or entire data is in accessed from the dictionary entry `train`. If the dataset is presplit, then the test data is accessed from the dictionary entry `test`.\n",
    "\n",
    "If it is not presplit, we will go ahead and manually split it. In this case, we use the `take()` and `skip()` method to split off 1000 examples from the train data into the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test (holdout), if not already.\n",
    "\n",
    "train = data['train']\n",
    "      \n",
    "# Let's get the test (holdout) portion of the dataset\n",
    "try:\n",
    "    test = data['test']\n",
    "# Not pre-split, so let's split it ourself\n",
    "except:\n",
    "    # Let's use 10% of dataset for test\n",
    "    n_examples = info.splits['train'].num_examples\n",
    "    n_test = int(n_examples * 0.1)\n",
    "    test  = train.take(n_test)\n",
    "    train = train.skip(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Let's now do a quick inspection by looking at one example in the dataset, based on the dataset category.\n",
    "\n",
    "    - CV : show as an image\n",
    "    - NL : show as text\n",
    "    - ST : show as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tfds.as_numpy(train.take(1)):\n",
    "    if CATEGORY == 'CV':\n",
    "        if SUBCATEGORY == 'VI':\n",
    "            image = example['video'][0]\n",
    "        else:\n",
    "\n",
    "            image = example['image']\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        try:\n",
    "            label = example['label']\n",
    "        except:\n",
    "            label = example['labels']\n",
    "        plt.title('Class: ' + str(label))\n",
    "        plt.show()\n",
    "    elif CATEGORY == 'NL':\n",
    "        if SUBCATEGORY == 'TC':\n",
    "            text = example['hypothesis']\n",
    "        elif SUBCATEGORY == 'TR':\n",
    "            text = example['en']\n",
    "        else:\n",
    "            text = example['text']\n",
    "\n",
    "        print(text)\n",
    "        try:\n",
    "            print(example['label'])\n",
    "        except:\n",
    "            pass\n",
    "    elif CATEGORY == 'ST':\n",
    "        print(example['features'])\n",
    "        print(example['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
